#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title

\noun on
Assignment 1: Matrix Multiplication
\end_layout

\begin_layout Author

\noun on
02614 High-Performance Computing
\end_layout

\begin_layout Date
Due 6th of January 2012
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{1cm}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
Henning Weiss - s062407
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0.6cm}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
Rasmus Bo SÃ¸rensen - s072080
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0.6cm}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
Evangelia Kasapaki - s114598
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Summary
\end_layout

\begin_layout Standard
In this report we will implement three different matrix-matrix multiplication
 functions and evaluate the performance of the functions in the number of
 floating point operations per second.
 The functions use different algorithms for multiplying the two matrices.
 The performance evaluation is done using the Analyzer from the Sun Studio.
 We have visualized the results of the performance evaluation in graphs,
 which are presented in this report.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Many physical problems can be reduced to systems of linear equations, thus
 they can be solved efficiently with linear algebra.
 Matrix multiplications are very calculation intensive and therefore they
 are a good benchmark for HPC systems.
\end_layout

\begin_layout Standard
We are to implement and evaluate three different matrix-matrix multiplication
 function used in a HPC system.
 The three different implementations are:
\end_layout

\begin_layout Enumerate
A simple matrix multiplication function, with three nested loops.
\end_layout

\begin_layout Enumerate
Invoking the DGEMM function from the sun performance library.
\end_layout

\begin_layout Enumerate
A blocked matrix multiplication function, with the block size adjusted for
 the processor characteristics.
\end_layout

\begin_layout Standard
The functions are evaluated in MFLOP/s as a function of memory footprint,
 to give an idea of the efficiency and to compare.
\end_layout

\begin_layout Section
Theory
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sub:Matrix-multiplication"

\end_inset

Matrix multiplication
\end_layout

\begin_layout Standard
Multiplying two matrices 
\begin_inset Formula $C=A\times B$
\end_inset

 can be done by the following simple algorithm: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
C_{ij}=\sum_{k=1}^{p}A_{ik}\cdot B_{kj}\label{eq:a}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This algorithm has the time complexity 
\begin_inset Formula $\mathcal{O}(n^{3})$
\end_inset

, for 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 being squared matrices.
 Whereas an matrix addition has the time complexity 
\begin_inset Formula $\mathcal{O}(n^{2})$
\end_inset

, for 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 being squared matrices.
\end_layout

\begin_layout Subsection
Simple_mm
\end_layout

\begin_layout Standard
The simple_mm function uses the algorithm described in section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Matrix-multiplication"

\end_inset

.
 This algorithm is very easy to understand, mostly because it is similar
 to the way we multiply matrices by hand.
 The simple_mm is therefore a good baseline for evaluation the performance
 of other implementation.
 An easy optimization of our simple_mm is to check whether an element in
 the current row is zero, in this case there is no need to go through all
 the elements in the corresponding column.
\end_layout

\begin_layout Standard
The simple algorithm has three nested loops, the two outer loops can be
 interchanged and the algorithm still reaches the correct result.
 The compiler can interchange these loops to optimize the memory accesses
 for the cache.
\end_layout

\begin_layout Subsection
Block_mm
\end_layout

\begin_layout Standard
Block_mm function performs block matrix multiplication.
 Matrices 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are partitioned in a number of submatrices, i.e.
 blocks, and the product is evaluated involving only operations on the smaller
 submatrices.
 For block dimension size 
\begin_inset Formula $S$
\end_inset

 matrices 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are partitioned in blocks as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
A=\left[\begin{array}{cccc}
A{}_{11} & A_{12} & ... & A_{1r}\\
A_{21} & A_{22} & ... & A_{2r}\\
... & ... & ... & ...\\
A_{p1} & A_{p2} & ... & A_{pr}
\end{array}\right]
\]

\end_inset


\begin_inset Formula 
\[
B=\left[\begin{array}{cccc}
B_{11} & B_{12} & ... & B_{1q}\\
B_{21} & B_{22} & ... & B_{2q}\\
... & ... & ... & ...\\
B_{r1} & B_{r2} & ... & B_{rq}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
The multiplication product 
\begin_inset Formula $C$
\end_inset

 is formed as the 
\begin_inset Formula $p\times q$
\end_inset

 block elements matrix 
\begin_inset Formula 
\[
C=\left[\begin{array}{cccc}
C{}_{11} & C_{12} & ... & C_{1q}\\
C_{21} & C_{22} & ... & C_{2q}\\
... & ... & ... & ...\\
C_{p1} & C_{p2} & ... & C_{pq}
\end{array}\right]
\]

\end_inset

Each submatrix 
\begin_inset Formula $C_{ij}$
\end_inset

is evaluated as seen in equation
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "eq:a"

\end_inset

 varying k from 1 to r.
 
\end_layout

\begin_layout Standard
The function is implemented according to this algorithm and has two nested
 loops to get each submatrix product.
 For each product requires a loop and a matrix multiplication and an addition
 inside the loop.
 In total there are six nested loops.
\end_layout

\begin_layout Standard
Since block matrix multiplication is only performed on submatrices it takes
 advantage of data locality.
 Considering the memory hierarchy and size, it is expected to be faster
 than the simple version.
 By choosing a good block size the memory hierarchy could be effectively
 exploited resulting in less cache misses and better performance.
\end_layout

\begin_layout Standard
The block size is expected to give a better performance when two input blocks
 fit in the cache at once.
 This is the case when matrices are larger than the cache size, for small
 sizes of matrices the block size does not affect the performance.
 
\end_layout

\begin_layout Subsection
DGEMM_mm
\end_layout

\begin_layout Standard
The DGEMM function is a highly optimized matrix-matrix multiplication function
 written in FORTRAN.
 FORTRAN saves data in multidimensional arrays column-wise compared to the
 row-wise manner of C.
 Because we are writing in C our wrapper must take care of the conversion
 between the C-style and the FORTRAN-style matrices.
 A matrix constructed in C style is interpreted as the transposed in FORTRAN.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
A_{C} & = & A_{F}^{T}\nonumber \\
B_{C} & = & B_{F}^{T}\label{eq:1}\\
C_{C} & = & C_{F}^{T}\nonumber 
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
The FORTRAN function takes the three input arguments A, B and C.
 Transposing the output of the function to get the desired matrix implies:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
C_{F} & = & A_{F}\times B_{F}\\
C_{F}^{T} & = & (A_{F}\times B_{F})^{T}\\
C_{F}^{T} & = & B_{F}^{T}\times A_{F}^{T}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Using the equations
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1"

\end_inset

 gives us the favorable result:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C_{C}=B_{C}\times A_{C}
\]

\end_inset


\end_layout

\begin_layout Standard
Which means that we only need to swap the parameters of the FORTRAN function.
 
\end_layout

\begin_layout Section
Experimental setup
\end_layout

\begin_layout Standard
We have used the Sun studio collect tool to generate our results.
 The collect tool allows us to get a summary of time spent in each function
 without the need for modifications to the program, enabling us to get fairly
 good performance measures.
 Additionally the tool allows us to query the hardware performance counters
 of the CPU to obtain supplemental performance data.
 We have used this facility to query the 
\begin_inset Quotes eld
\end_inset

fp_ops
\begin_inset Quotes erd
\end_inset

 (total floating point operations) and the data cache misses counters.
\end_layout

\begin_layout Standard
The matrix multiplication subroutines must run for a couple of seconds to
 allow 
\emph on
collect
\emph default
 to query the program and get the measurements.
 For this purpose we have created loops around the matrix calculation functions
 in our program that achieve a runtime of at least three seconds for each
 of the three algorithms.
 The number of times the algorithms runs may vary, but by normalizing the
 performance counters with the actual time spent we can generate comparable
 results.
 As a positive side effect this will give us an average for all data generated
 of our functions.
\end_layout

\begin_layout Standard
Our main program several parameters as input, allowing us to control the
 size of the matrices and the block size used by the block matrix-multiply
 algorithm.
 This allowed us to write a wrapper script that varies the size of the matrices
 generated, making it easy to generate many measurements.
 
\emph on
Collect
\emph default
 will generate files binary files that can be accessed by 
\emph on
er_print
\emph default
 and further manipulation by bash scripts, awk and gnuplot allows us to
 fully automatically generate performance graphs, which are presented in
 the following sections.
\end_layout

\begin_layout Section
Machine description
\end_layout

\begin_layout Standard
We used two types of machines for the performance evaluation, AMD and Intel.
 Both machines are 64-bits processors of x86 architecture.
 The AMD machine has 2 AMD Opteron 6168 processors with a total of 24 cores
 reaching 0.8-1.9GHz.
 It has 64KB of L1 cache and 512KB of L2 on each core.
 It also has 12MB L3 cache on each processor.
 Intel machine has 2 Intel Xeon X5550 processors with a total of 8 cores
 reaching 1.6-2.67GHZ.
 It has 32KB of L1 cache and 256KB of L2 cache on each core.
 It also has a L3 cache of 8MB per processor.
 The both have separate instruction and data L1 caches, while cache is unified
 in L2 and L3.
 Both machines run Linux 2.6.32 as OS.
\end_layout

\begin_layout Standard
For the experiments on performance evaluation on variable matrix size we
 used the AMD machine.
 For variable block size we used both machines.
 The difference in the memory hierarchy sizes is considered to affect the
 performance of the function by changing the block size.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Machine configuration for the performed experiments.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="7">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Vendor
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
OS
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Arch
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
# of cores
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
L1 Cache
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
L2 Cache
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Clock
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
AMD
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Linux 2.6.32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
x86_64
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
24
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64kb
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
512kb
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.9 GHz
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Intel
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Linux 2.6.32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
x84_64
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
32kb
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
256kb
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2.67 GHz
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Compilers
\end_layout

\begin_layout Standard
We compile our program using the Sun C 5.10 Linux_i386(suncc) compiler using
 the following options
\emph on
 -fast, -xlic_lib=sunperf
\emph default
, 
\emph on
-xrestrict, -xO5 
\emph default
flags.
 Additionally we use the 
\emph on
-xchip=opteron
\emph default
 flag on the Linux AMD machines to get the chip set optimizations.
 Fast is a macro that enables many other optimization options and the 
\emph on
-xO5
\emph default
 enables the highest optimization level to enable good performance.
 The 
\emph on
-xlic_lib=sunperf 
\emph default
specifies the linking against the Sun performance library which is used
 for 
\emph on
DGEMM
\emph default
.
 Finally the 
\emph on
-xrestrict 
\emph default
flags tells the compiler that we have no overlapping pointers in our code.
 Overlapping pointers prevent the compiler from doing loop unrolling.
\end_layout

\begin_layout Standard
When using the Sun studio analyzer, it is shown that the row and column
 loops in our functions are interchanged and the loops are unrolled up to
 eight times to increase performance.
 Additionally pre-fetching is enabled automatically in the compiler, allowing
 us to circumvent some of the cache misses.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
We now evaluate the three matrix-matrix multiplication functions we have
 implemented.
 To evaluate the performance of the block_mm we need to find a good block
 size for the cache.
 To find a good block size for the block_mm we vary the block size for a
 fixed size matrix-matrix multiplication.
 In figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Varying-the-dimension"

\end_inset

 we have plotted the performance of the block_mm when varying the dimension
 of the block size.
 For the experiments we used a fixed matrix of size 
\begin_inset Formula $1000\times1000$
\end_inset

 we vary the block size from 10 to 150 in steps of 5.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename graph_blocks.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Varying-the-dimension"

\end_inset

Varying the dimension of the block of the block_mm.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Varying-the-dimension"

\end_inset

 we see that the AMD run has its maximum around 60.
 The block dimension of 60 allows for 2 block to be placed in the cache
 at once.
 This comes from the fact that AMD has 64 Kb of L1 cache and two blocks.
 Two blocks requires: 
\begin_inset Formula 
\[
60\text{Â²[elements]}*8\text{[bytes]}*2\text{[blocks]}=57.600\text{[Kb]}
\]

\end_inset


\end_layout

\begin_layout Standard
When two blocks are placed in the L1 cache the program can do the multiplication
 of those two blocks right from the cache, writing the result back into
 memory reducing cache misses.
 
\end_layout

\begin_layout Standard
The AMD curve drops right after the block dimension reaches 65.
 This is because the two block no longer fits in the L1 cache.
 There is also a second drop right after the block dimension reaches 85,
 which is where one block takes up the entire L1 cache.
\end_layout

\begin_layout Standard
The Intel curve follows the same trend as the AMD curve, but with less variation.
 The reduced variation could be explained by a better cache prefetching
 of the Intel processor.
\end_layout

\begin_layout Standard
With the block size set to 60 we now evaluate the performance of all the
 three functions.
 The performance of the three functions can be seen in figure
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "fig:flops"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename graph_flops.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:flops"

\end_inset

Floating point performance of the three matrix-matrix multiplication implementat
ions.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We can see that the library function DGEMM outperforms our homemade implementati
ons by far.
 This is because the DGEMM function is highly optimized.
 The simple_mm performs slightly better than the block_mm for smaller matrix
 sizes.
 When the matrix sizes become larger the block_mm becomes more efficient
 than the simple_mm.
 The extra overhead in block_mm, in form of more nested loops and a larger
 number of branches, could explain the performance difference between simple_mm
 and block_mm for small matrix sizes.
\end_layout

\begin_layout Standard
We can see that our own implementations will never be as efficient as the
 DGEMM library function.
\end_layout

\begin_layout Standard
The performance of simple_mm relates to the size of the different cache
 levels.
 The performance can be seen to drop at the cache level sizes.
 Even though the drop for the L1 cache is very insignificant.
\end_layout

\begin_layout Standard
To see how the implementations performs
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename graph_cache.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Cache misses of the three matrix-matrix multiplication implementations.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
When analyzing the DGEMM function we saw that number of floating point operation
s where reduced to around half compared to the simple_mm.
 Furthermore the cache misses for the DGEMM was drastically reduced compared
 to the simple_mm function, we see the cache efficiency and the reduced
 number of floating point operations as the source of the performance advantage
 of around 10 times.
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
We have shown the three different implementation of the matrix-matrix multiplica
tion functions, and evaluated the performance of them.
 We have shown that our implementations of the function cannot compete with
 a highly optimized library function of an HPC library.
 However different implementations that take advantage of data locality
 and machine properties can affect performance.
 The blocking matrix-matrix multiplication algorithm is faster for greater
 matrix sizes, but it never reaches the performance of the DGEMM library
 function.
\end_layout

\end_body
\end_document
